import os
import datetime
import requests
import feedparser

SAMPLES_DIR = "./samples"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36"
}

def _ensure_dir(path):
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)

def save_text(fname: str, text: str):
    """ç»Ÿä¸€çš„å†™æ–‡ä»¶å‡½æ•°ï¼Œæ”¾åˆ° /samples ä¸‹"""
    fpath = os.path.join(SAMPLES_DIR, fname)
    with open(fpath, "w", encoding="utf-8") as f:
        f.write(text)
    print(f"[OK] å†™å…¥ {fpath}")

# ----------------------------------------------------
# 1. Hugging Face Blog
# ----------------------------------------------------
def fetch_hf_blog(limit: int = 5):
    """
    æŠ“ Hugging Face Blog æœ€æ–°å‡ ç¯‡
    ç­–ç•¥ï¼š
      1. å…ˆæŠ“åˆ—è¡¨é¡µ https://huggingface.co/blog
      2. è§£æå‡ºå‰ limit æ¡çš„æ ‡é¢˜ + é“¾æ¥
      3. å°è¯•é€ç¯‡è¯·æ±‚è¯¦æƒ…é¡µï¼ŒæŠŠå‰ 2~3 æ®µæ­£æ–‡æŠ“å‡ºæ¥
      4. æŠ“ä¸åˆ°æ­£æ–‡å°±å†™â€œæ— æ‘˜è¦ï¼ˆè¯·ç‚¹å‡»é˜…è¯»å…¨æ–‡ï¼‰â€
    """
    url = "https://huggingface.co/blog"
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
    except Exception as e:
        print("[HF] æŠ“å–å¤±è´¥ï¼š", e)
        return

    from bs4 import BeautifulSoup  # éœ€è¦ pip install beautifulsoup4
    soup = BeautifulSoup(resp.text, "html.parser")

    # HF é¡µé¢ç»“æ„å¯èƒ½ä¼šå˜ï¼Œè¿™é‡Œåšä¸ªæ¯”è¾ƒå®½æ¾çš„é€‰å–
    articles = soup.find_all("a", href=True)
    items = []
    for a in articles:
        href = a["href"]
        # ç­›ä¸€ä¸‹çœŸæ­£çš„ blog é“¾æ¥
        if href.startswith("/blog/"):
            title = a.get_text(strip=True)
            if not title:
                continue
            full_link = "https://huggingface.co" + href
            items.append((title, full_link))
    # å»é‡ & æˆªæ–­
    seen = set()
    uniq = []
    for t, l in items:
        if l in seen:
            continue
        seen.add(l)
        uniq.append((t, l))
    uniq = uniq[:limit]

    today = datetime.date.today().isoformat()
    lines = [f"ğŸ¤— Hugging Face Blog æœ€æ–° {len(uniq)} ç¯‡æ–‡ç«  ({today})", ""]

    # å†é€ç¯‡æŠ“æ­£æ–‡
    for idx, (title, link) in enumerate(uniq, start=1):
        lines.append(f"### {idx}. {title}")
        lines.append(f"é“¾æ¥: {link}")

        summary = "æ— æ‘˜è¦ï¼ˆè¯·ç‚¹å‡»é˜…è¯»å…¨æ–‡ï¼‰"
        try:
            detail = requests.get(link, headers=HEADERS, timeout=15)
            if detail.status_code == 200:
                dsoup = BeautifulSoup(detail.text, "html.parser")
                # æ‰¾æ­£æ–‡å®¹å™¨ï¼ŒHF ç»å¸¸ç”¨ <article> æˆ– markdown-content ä¹‹ç±»çš„ class
                paras = dsoup.find_all("p")
                paras_text = [p.get_text(" ", strip=True) for p in paras if p.get_text(strip=True)]
                if paras_text:
                    # åªå–å‰ 2 æ®µï¼Œé¿å…æ–‡ä»¶å¤ªå¤§
                    summary = "\n".join(paras_text[:2])
        except Exception as e:
            print(f"[HF] æŠ“æ­£æ–‡å¤±è´¥ {link}: {e}")

        lines.append(f"æ‘˜è¦: {summary}")
        lines.append("")  # ç©ºè¡Œ

    save_text(f"{today}_huggingface_blog.txt", "\n".join(lines))

# ----------------------------------------------------
# 2. arXiv
# ----------------------------------------------------
def fetch_arxiv_ai(limit: int = 10):
    """
    æ‹‰ arXiv ä¸Š cs.AI / cs.CL / cs.LG æœ€è¿‘çš„æ–‡ç« æ ‡é¢˜+æ‘˜è¦
    """
    url = (
        "http://export.arxiv.org/api/query?"
        "search_query=cat:cs.AI+OR+cat:cs.CL+OR+cat:cs.LG&"
        f"start=0&max_results={limit}&sortBy=submittedDate&sortOrder=descending"
    )
    feed = feedparser.parse(url)

    today = datetime.date.today().isoformat()
    lines = [f"ğŸ“˜ arXiv æœ€æ–° {limit} ç¯‡ AI/ML è®ºæ–‡ ({today})", ""]

    for entry in feed.entries:
        title = entry.title
        summary = entry.summary
        link = entry.link
        lines.append(f"### {title}")
        lines.append(f"é“¾æ¥: {link}")
        lines.append(f"æ‘˜è¦: {summary}")
        lines.append("")

    save_text(f"{today}_arxiv_ai_trends.txt", "\n".join(lines))


# ----------------------------------------------------
# 4. Meta AI Blog
# ----------------------------------------------------
def fetch_meta_ai_blog(limit: int = 5):
    """
    Meta AI Blog åœ¨ä½ è¿™é‡Œ 403ï¼Œæˆ‘ä»¬å…ˆå†™ä¸ªå…œåº•çš„ï¼Œ
    è‡³å°‘è®©ä½ çš„æœ¬åœ°çŸ¥è¯†åº“é‡Œæœ‰â€œMeta AI Blogâ€è¿™ä¸ªåè¯ï¼Œæ–¹ä¾¿æ£€ç´¢ã€‚
    """
    today = datetime.date.today().isoformat()
    lines = [
        f"ğŸ§  Meta AI Blog æœ€æ–° {limit} ç¯‡ï¼ˆå ä½æ•°æ®ï¼Œå› ä¸º https://ai.meta.com/blog/ è¿”å› 403ï¼‰",
        "",
        "è¿™ä¸ªç«™ç‚¹å½“å‰ä¸èƒ½ç”¨ requests ç›´æ¥æŠ“ï¼Œæˆ‘ä»¬å…ˆæŠŠå®˜æ–¹åšå®¢å…¥å£å†™è¿›å»ï¼Œ",
        "ç­‰èƒ½è®¿é—®æ—¶å†æ›´æ–°ä¸ºçœŸå®å†…å®¹ã€‚",
        ""
    ]

    # ç»™ä½ å‡ ä¸ªå¸¸è§çš„ meta ai æ–‡ç« æ ‡é¢˜åšä¸ªâ€œå‡çš„ç›®å½•â€ï¼Œè‡³å°‘èƒ½è¢«æœåˆ°
    fake_posts = [
        ("The Latest", "https://ai.meta.com/blog/"),
        ("LLaMA ç³»åˆ—æ¨¡å‹æ›´æ–°", "https://ai.meta.com/blog/"),
        ("Segment Anything / SAM ç›¸å…³è¿›å±•", "https://ai.meta.com/blog/"),
        ("Multimodal / embodied AI ç ”ç©¶", "https://ai.meta.com/blog/"),
        ("Meta GenAI äº§å“ä¸ç ”ç©¶è·¯çº¿", "https://ai.meta.com/blog/"),
    ]

    for i, (title, link) in enumerate(fake_posts[:limit], start=1):
        lines.append(f"### {i}. {title}")
        lines.append(f"é“¾æ¥: {link}")
        lines.append("æ‘˜è¦: å½“å‰æ— æ³•è·å–æ‘˜è¦ï¼ˆ403ï¼‰ï¼Œè¯·æ‰‹åŠ¨æ‰“å¼€é“¾æ¥æŸ¥çœ‹åŸæ–‡ã€‚")
        lines.append("")

    return "\n".join(lines)


# ----------------------------------------------------
# 5. DeepMind Blog
# ----------------------------------------------------
def fetch_deepmind_blog(limit: int = 5):
    url = "https://deepmind.google/discover/blog/"
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
    except Exception as e:
        print("[DeepMind] æŠ“å–å¤±è´¥ï¼š", e)
        return ""

    from bs4 import BeautifulSoup
    soup = BeautifulSoup(resp.text, "html.parser")

    posts = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "/discover/blog/" in href:
            title = a.get_text(strip=True)
            if title:
                if href.startswith("http"):
                    link = href
                else:
                    link = "https://deepmind.google" + href
                posts.append((title, link))

    posts = posts[:limit]

    today = datetime.date.today().isoformat()
    lines = [f"ğŸŸ£ DeepMind Blog æœ€æ–° {len(posts)} ç¯‡ ({today})", ""]

    for i, (title, link) in enumerate(posts, start=1):
        summary = "æ— æ‘˜è¦ï¼ˆè¯·ç‚¹å‡»é˜…è¯»å…¨æ–‡ï¼‰"
        try:
            detail = requests.get(link, headers=HEADERS, timeout=15)
            if detail.status_code == 200:
                dsoup = BeautifulSoup(detail.text, "html.parser")
                ps = dsoup.find_all("p")
                if ps:
                    summary = "\n".join(p.get_text(" ", strip=True) for p in ps[:2])
        except Exception as e:
            print("[DeepMind] æŠ“æ­£æ–‡å¤±è´¥ï¼š", e)

        lines.append(f"### {i}. {title}")
        lines.append(f"é“¾æ¥: {link}")
        lines.append(f"æ‘˜è¦: {summary}")
        lines.append("")

    return "\n".join(lines)

# ----------------------------------------------------
if __name__ == "__main__":
    _ensure_dir(SAMPLES_DIR)
    today = datetime.date.today().isoformat()

    # 1) HF
    fetch_hf_blog()

    # 2) arXiv
    fetch_arxiv_ai()

    # 4) Meta
    meta_text = fetch_meta_ai_blog()
    if meta_text:
        save_text(f"{today}_meta_ai_blog.txt", meta_text)

    # 5) DeepMind
    deep_text = fetch_deepmind_blog()
    if deep_text:
        save_text(f"{today}_deepmind_blog.txt", deep_text)

    print("\nâœ… å…¨éƒ¨æŠ“å–å®Œæˆï¼Œå¯ä»¥å» ./samples é‡Œçœ‹äº†ï¼Œç„¶åå†è·‘ï¼š")
    print("   python main.py --ingest ./samples")
