import os
import argparse
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from memory.vector_store import VectorStore
from retriever.pdf_loader import load_pdf_dir
from retriever.md_loader import load_md_dir
from tools.search_tool import search
from tools.translate_tool import translate
from tools.calc_tool import safe_eval

load_dotenv()

MODEL_NAME = os.getenv("MODEL_NAME", "gpt-4o-mini")

class SmartKnowledgeAgent:
    def __init__(self):
        self.llm = ChatOpenAI(model=MODEL_NAME, temperature=0.2)
        self.embeddings = OpenAIEmbeddings()
        self.vs = VectorStore(self.embeddings)

         # ğŸ‘‡ å…³é”®ï¼šå¯åŠ¨å°±å°è¯•æŠŠæœ¬åœ°å‘é‡åº“è¯»å›æ¥
        try:
            self.vs.load()
            print("[DEBUG] å·²åŠ è½½æœ¬åœ°å‘é‡åº“ã€‚")
        except Exception as e:
            print("[DEBUG] æš‚æ— å·²æœ‰å‘é‡åº“ï¼Œå°†åœ¨ä¸‹ä¸€æ¬¡ ingest æ—¶åˆ›å»ºï¼š", e)
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=800, chunk_overlap=120, separators=["\n\n", "\n", ". "]
        )

    def ingest(self, paths):
        docs = []
        for p in paths:
            if os.path.isdir(p):
                docs += load_pdf_dir(p)
                docs += load_md_dir(p)
            elif p.lower().endswith(".pdf"):
                docs += load_pdf_dir(os.path.dirname(p) or ".", only_file=os.path.basename(p))
            elif p.lower().endswith((".md", ".txt")):
                docs += load_md_dir(os.path.dirname(p) or ".", only_file=os.path.basename(p))
        if not docs:
            print("[Ingest] æœªæ‰¾åˆ°å¯ç”¨æ–‡æ¡£")
            return
        chunks = self.splitter.split_documents([Document(page_content=d.page_content, metadata=d.metadata) for d in docs])
        self.vs.build(chunks)
        self.vs.save("./memory")
        print(f"[DEBUG] æ”¶åˆ°è·¯å¾„: {paths}")
        print(f"[DEBUG] è¯»å–æ–‡æ¡£æ•°: {len(docs)}")
        print(f"[DEBUG] æ‹†åˆ†å—æ•°: {len(chunks)}")

        print(f"[Ingest] å·²æ„å»ºå‘é‡åº“ï¼š{len(chunks)} chunks")

    def rag_answer(self, query: str, k: int = 5):
        # 1. æ£€ç´¢
        refs = self.vs.search(query, k=k)

        # 2. æ‰“å°ä¸€ä¸‹æ£€ç´¢ç»“æœï¼Œæ–¹ä¾¿è°ƒè¯•
        print("[DEBUG] æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µï¼š")
        for i, r in enumerate(refs):
            print(f"  - Top-{i+1} æ¥æº: {r.metadata.get('source','unknown')}")
            print(f"    å†…å®¹å‰60å­—: {r.page_content[:60]!r}")

        # 3. æ‹¼ä¸Šä¸‹æ–‡
        context = "\n\n".join([r.page_content for r in refs])

        # 4. è°ƒç”¨ LLM
        sys_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„çŸ¥è¯†åŠ©æ‰‹ã€‚è¯·ä¸»è¦ä¾æ®â€œä¸Šä¸‹æ–‡â€æ¥å›ç­”ï¼›"
            "å¦‚æœä¸Šä¸‹æ–‡ç¡®å®ä¸è¶³ï¼Œå°±è¯´æ˜ä¸è¶³ï¼Œä¸è¦èƒ¡ç¼–ã€‚"
            "æœ€åè¯·è¾“å‡ºä¸€ä¸ªâ€œå‚è€ƒæ¥æºâ€åˆ—è¡¨ã€‚"
        )
        user_prompt = f"é—®é¢˜ï¼š{query}\n\nä¸Šä¸‹æ–‡ï¼š\n{context}"
        resp = self.llm.invoke([
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": user_prompt},
        ])

        # 5. æŠŠå¼•ç”¨æ‹¼åˆ°æœ€å
        citations_lines = []
        for r in refs:
            src = r.metadata.get("source", "unknown")
            citations_lines.append(f"- {src}")
        citations = "\n".join(citations_lines) if citations_lines else "- ï¼ˆæ— æ£€ç´¢ç»“æœï¼‰"

        final_answer = resp.content + "\n\nå‚è€ƒæ¥æºï¼š\n" + citations
        return final_answer

        # >>> æ–°å¢ï¼šæ™ºèƒ½å›ç­”ï¼ˆæœ¬åœ°ä¼˜å…ˆ + ä¸è¶³å†è”ç½‘ï¼‰
    def smart_answer(self, query: str, k: int = 5) -> str:
    # """
    # æ™ºèƒ½ç‰ˆé—®ç­”ï¼ˆå¸¦è°ƒè¯•ï¼‰ï¼š
    # 1. å…ˆä»æœ¬åœ°å‘é‡åº“é‡ŒæŸ¥ k æ®µ
    # 2. çœ‹æœ¬åœ°å†…å®¹å¤Ÿä¸å¤Ÿï¼ˆé•¿åº¦ + æ•°é‡ï¼‰
    # 3. ä¸å¤Ÿå°±å»è”ç½‘æœç´¢
    # 4. æœ€åæŠŠæœ¬åœ° + è”ç½‘ ä¸€èµ·å–‚ç»™ llm
    # """
        DEBUG_RAG = True  # æ‰“å¼€/å…³é—­è°ƒè¯•è¾“å‡º
        if DEBUG_RAG:
            print("\n" + "=" * 80)
            print(f"[RAG DEBUG] ç”¨æˆ·é—®é¢˜ï¼š{query}")

        # â‘  å…ˆæŸ¥æœ¬åœ°
        refs = self.vs.search(query, k=k)   # è¿™é‡Œå°±æ˜¯ä½ çš„å‘é‡åº“æ£€ç´¢
        local_texts = []   # åªåœ¨ä¸‹é¢å¾ªç¯é‡Œæ„å»ºï¼Œé¿å…é‡å¤
        

        # æ‰“å°æœ¬åœ°æ£€ç´¢è¯¦æƒ…
        if DEBUG_RAG:
            print(f"[RAG DEBUG] æœ¬åœ°æ£€ç´¢è¿”å› {len(refs)} æ®µï¼š")
        for i, r in enumerate(refs, start=1):
            src = r.metadata.get("source") if hasattr(r, "metadata") else f"æ ·æœ¬_{i}"
            chunk_text = f"[{i}] æ¥æº: {src}\n{r.page_content}"
            local_texts.append(chunk_text)
            if DEBUG_RAG:
                preview = r.page_content.replace("\n", " ")[:120]
                print(f"  [{i}] æ¥æº: {src}")
                print(f"      å†…å®¹: {preview}...")
        # â‘¡ åˆ¤æ–­æœ¬åœ°æ˜¯å¦è¶³å¤Ÿï¼ˆæ–°è§„åˆ™ï¼‰
        # >>> ä¿®æ”¹å¼€å§‹ï¼šæ–°çš„è”ç½‘åˆ¤å®šé€»è¾‘ï¼ˆç›¸ä¼¼åº¦é˜ˆå€¼ + å æ¯” + å…³é”®è¯è¦†ç›–ï¼‰

        import os, re
        from statistics import median

        # å¯é€šè¿‡ç¯å¢ƒå˜é‡è°ƒå‚ï¼›æ²¡æœ‰å°±ç”¨é»˜è®¤å€¼
        SIM_TAU   = float(os.getenv("RAG_SIM_TAU", "0.52"))  # å•æ®µâ€œé è°±â€çš„ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆâ‰¥ è®¤ä¸ºç›¸å…³ï¼‰
        MIN_GOOD  = int(os.getenv("RAG_MIN_GOOD", "3"))      # è‡³å°‘éœ€è¦è¿™ä¹ˆå¤šâ€œé è°±æ®µâ€
        MIN_PROP  = float(os.getenv("RAG_MIN_PROP", "0.40")) # é è°±æ®µå æ¯”ä¸‹é™

        def _cn_keywords(q: str, topn: int = 6):
            # ä»ä¸­æ–‡é—®é¢˜é‡Œæå–é•¿åº¦â‰¥2çš„è¯ç‰‡æ®µï¼›è‹±æ–‡é—®é¢˜å¯ä»¥è‡ªå·±æ‰©å±•
            return re.findall(r"[\u4e00-\u9fa5]{2,}", q)[:topn]

        # å–æ£€ç´¢åˆ†æ•°ï¼ˆä¸åŒåç«¯æœ‰çš„æ”¾åœ¨ r.scoreï¼Œæœ‰çš„æ”¾åœ¨ r.metadata["score"]ï¼Œéƒ½å…¼å®¹ï¼‰
        scores = []
        for r in refs:
            s = getattr(r, "score", None)
            if s is None and hasattr(r, "metadata"):
                s = r.metadata.get("score")
            try:
                s = float(s) if s is not None else 0.0
            except Exception:
                s = 0.0
            scores.append(s)

        good_idx = [i for i, s in enumerate(scores) if s >= SIM_TAU]
        prop = len(good_idx) / max(len(refs), 1)

        # å…³é”®è¯è¦†ç›–ï¼šå‰ 8 æ®µé‡Œè‡³å°‘å‡ºç°è¿‡ä¸€ä¸ªå…³é”®è¯ï¼Œé¿å…è¢«â€œAI/æ–‡åŒ–â€è¿™ç±»æ³›æ®µè¯¯åˆ¤ä¸ºç›¸å…³
        kws = _cn_keywords(query)
        kw_hits = sum(
            1 for r in refs[:min(8, len(refs))]
            if (kws and any(kw in r.page_content for kw in kws))
        )

        # ç»¼åˆåˆ¤å®šï¼šæ»¡è¶³â€œé è°±æ®µæ•°é‡ + å æ¯” + å…³é”®è¯è¦†ç›–â€æ‰è®¤ä¸ºæœ¬åœ°è¶³å¤Ÿ
        has_enough_local = (
            (len(good_idx) >= MIN_GOOD) and
            (prop >= MIN_PROP) and
            (kw_hits > 0 or not kws)     # æ²¡æœ‰ä¸­æ–‡å…³é”®è¯æ—¶æ”¾å®½è¿™ä¸€æ¡
        )

        if DEBUG_RAG:
            max_score = max(scores) if scores else 0.0
            med_score = median(scores) if scores else 0.0
            print(f"[RAG DEBUG] æœ¬åœ°æ£€ç´¢åˆ†æ•°: max={max_score:.3f}  median={med_score:.3f}  "
                f"good={len(good_idx)}  prop={prop:.2f}  kw_hits(top8)={kw_hits}")
            print(f"[RAG DEBUG] åˆ¤å®šæœ¬åœ°æ˜¯å¦è¶³å¤Ÿ: {has_enough_local}")

        # <<< ä¿®æ”¹ç»“æŸï¼šæ–°çš„è”ç½‘åˆ¤å®šé€»è¾‘


        # â‘¢ ä¸å¤Ÿå°±å»è”ç½‘
        web_text = ""
        if not has_enough_local:
            if DEBUG_RAG:
                print("[RAG DEBUG] â†’ åˆ¤å®šä¸ºæœ¬åœ°èµ„æ–™ä¸è¶³ï¼Œå‡†å¤‡è°ƒç”¨è”ç½‘æœç´¢ ...")
            raw_web_text = search(query)                 # ä½ çš„æœç´¢å‡½æ•°
            zh = translate(raw_web_text)                 # ç¿»è¯‘æˆä¸­æ–‡

            # ç»™è”ç½‘ç»“æœåŠ ä¸Šâ€œæ ‡é¢˜ + é“¾æ¥ + æ‘˜è¦â€çš„å£³ï¼Œä¾¿äºæ¨¡å‹å¼•ç”¨
            web_text = (
                f"### è”ç½‘æ£€ç´¢æ‘˜è¦ï¼š{query}\n"
                f"é“¾æ¥ï¼šæ— ï¼ˆDuckDuckGo æ‘˜è¦ï¼‰\n"
                f"æ‘˜è¦ï¼š{zh}\n"
            )
            if DEBUG_RAG:
                print(f"[RAG DEBUG] è”ç½‘è¿”å›å†…å®¹é•¿åº¦: {len(web_text)}")
        else:
            if DEBUG_RAG:
                print("[RAG DEBUG] â†’ æœ¬åœ°èµ„æ–™å·²è¶³å¤Ÿï¼Œæœ¬æ¬¡ä¸è”ç½‘ã€‚")


        # â‘£ ç»„ç»‡æœ€ç»ˆ prompt
        context_blocks = []
        if web_text:   # å…ˆæ”¾è”ç½‘ï¼Œè®©é¢„è§ˆèƒ½çœ‹åˆ°
            context_blocks.append("ã€è”ç½‘ç»“æœã€‘\n" + web_text)
        if local_texts:
            context_blocks.append("ã€æœ¬åœ°èµ„æ–™ã€‘\n" + "\n\n".join(local_texts))

        final_context = "\n\n".join(context_blocks) if context_blocks else "ï¼ˆæ— èµ„æ–™ï¼‰"

        if DEBUG_RAG:
            print("[RAG DEBUG] æœ€ç»ˆé€å…¥ LLM çš„ä¸Šä¸‹æ–‡ï¼ˆå‰ 400 å­—æ¯å—ï¼‰ï¼š")
            for blk in context_blocks:
                print("-" * 40)
                print(blk[:400])  # æ¯å—å„æˆª 400 å­—
            print("-" * 40)


        # â‘¤ è°ƒ LLM
        sys_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢å¢å¼ºé—®ç­”åŠ©æ‰‹ã€‚\n"
            "ä½ åªèƒ½æ ¹æ®ç»™ä½ çš„èµ„æ–™ä½œç­”ï¼Œä¸è¦è‡ªå·±ç¼–èµ„æ–™ã€‚\n"
            "å¿…é¡»åœ¨å›ç­”ç»“å°¾ç»™å‡ºâ€œå‚è€ƒæ¥æºâ€è¿™ä¸€æ®µï¼ŒæŠŠä½ ç”¨åˆ°çš„æœ¬åœ°/è”ç½‘ç‰‡æ®µéƒ½åˆ—å‡ºæ¥ã€‚\n"
            "å¦‚æœèµ„æ–™ç¡®å®æ²¡æœ‰ï¼Œå°±è¯´â€œèµ„æ–™é‡Œæ²¡æœ‰ç›¸å…³å†…å®¹â€ï¼Œä¸è¦ççŒœã€‚\n"
            # """ä½ æ˜¯æ£€ç´¢å¢å¼ºåŠ©æ‰‹ã€‚åªå…è®¸ä½¿ç”¨æˆ‘æä¾›çš„ã€ä¸Šä¸‹æ–‡ã€‘ä½œç­”ï¼š
            # - å¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ä»¥å›ç­”ï¼Œè¯·æ˜ç¡®å‘Šè¯‰æˆ‘â€œæœªæ‰¾åˆ°ç›¸å…³èµ„æ–™â€ï¼›
            # - ä¸è¦å‡­å¸¸è¯†æˆ–å¤–éƒ¨è®°å¿†ç¼–é€ ç­”æ¡ˆï¼›
            # - å¦‚ä½¿ç”¨äº†èµ„æ–™ï¼Œè¯·åœ¨ç»“å°¾åˆ—å‡ºâ€œå‚è€ƒæ¥æºï¼š<æ–‡ä»¶æˆ–é“¾æ¥>â€ã€‚"""
        )

        user_prompt = (
            f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\n"
            "ä¸‹é¢æ˜¯ä½ èƒ½ç”¨åˆ°çš„èµ„æ–™ï¼š\n"
            f"{final_context}\n\n"
            "è¯·å…ˆç»™å‡ºç®€æ´å›ç­”ï¼Œç„¶åæ¢è¡Œå†™ï¼š\n"
            "å‚è€ƒæ¥æºï¼š\n"
            "- åˆ—å‡ºä½ ä¸Šé¢ç”¨åˆ°çš„èµ„æ–™æ ‡é¢˜/æ–‡ä»¶å/URLèƒ½çœ‹è§çš„é‚£ä¸€æ®µï¼›\n"
            "- å¦‚æœåªæœ‰ä¸€ä»½èµ„æ–™ï¼Œå°±å†™ä¸€ä¸ªï¼›\n"
            "- å¦‚æœå®Œå…¨æ²¡èµ„æ–™ï¼Œå°±å†™â€œï¼ˆæ— ï¼‰â€ã€‚\n"
        )

        ans = self.llm.invoke([
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": user_prompt},
        ]).content

        if DEBUG_RAG:
            print("[RAG DEBUG] LLM æœ€ç»ˆå›ç­”ï¼š")
            print(ans)
            print("=" * 80 + "\n")

        return ans


    def call_tool(self, name: str, **kwargs):
        if name == "search":
            return search(kwargs.get("q", ""))
        if name == "translate":
            return translate(kwargs.get("text", ""), kwargs.get("target", "zh"))
        if name == "calc":
            return safe_eval(kwargs.get("expr", ""))
        return f"æœªçŸ¥å·¥å…·ï¼š{name}"


def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--ingest", nargs="*", help="æ–‡æ¡£è·¯å¾„æˆ–ç›®å½•ï¼ˆå¯å¤šé€‰ï¼‰")
    ap.add_argument("--ask", type=str, help="é—®ç­”é—®é¢˜", default=None)
    # >>> æ–°å¢ï¼šå‘½ä»¤è¡Œé‡Œä¹Ÿå¯ä»¥èµ°æ™ºèƒ½å›ç­”
    ap.add_argument("--smart-ask", type=str, help="æ™ºèƒ½é—®ç­”ï¼ˆæœ¬åœ°+è”ç½‘ï¼‰", default=None)
    ap.add_argument("--tool", type=str, help="è°ƒç”¨å·¥å…·ï¼šsearch/translate/calc", default=None)
    ap.add_argument("--tool-args", type=str, help="å·¥å…·å‚æ•°ï¼Œç¤ºä¾‹ï¼šq=langchain expr=1+2*3", default=None)
    return ap.parse_args()


def parse_kv(s):
    if not s:
        return {}
    out = {}
    for kv in s.split():
        if "=" in kv:
            k, v = kv.split("=", 1)
            out[k] = v
    return out


if __name__ == "__main__":
    args = parse_args()
    agent = SmartKnowledgeAgent()

    if args.ingest:
        agent.ingest(args.ingest)

    if args.ask:
        print("\n[RAG Answer]\n" + agent.rag_answer(args.ask))

    # >>> æ–°å¢ï¼šå‘½ä»¤è¡Œæµ‹è¯•æ™ºèƒ½é—®ç­”
    if args.smart_ask:
        print("\n[Smart Answer]\n" + agent.smart_answer(args.smart_ask))

    if args.tool:
        kwargs = parse_kv(args.tool_args)
        print("\n[Tool Result]\n", agent.call_tool(args.tool, **kwargs))
